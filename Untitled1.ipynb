{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4fd0a6-dd8a-4a0a-880c-ff4baa774af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch:  0 loss:  85.75841522216797\n",
      "epoch:  1 loss:  83.7375259399414\n",
      "epoch:  2 loss:  81.79911041259766\n",
      "epoch:  3 loss:  79.80228424072266\n",
      "epoch:  4 loss:  77.5121078491211\n",
      "epoch:  5 loss:  74.79102325439453\n",
      "epoch:  6 loss:  71.53963470458984\n",
      "epoch:  7 loss:  67.6519546508789\n",
      "epoch:  8 loss:  63.076297760009766\n",
      "epoch:  9 loss:  57.76588821411133\n",
      "epoch:  10 loss:  51.644683837890625\n",
      "epoch:  11 loss:  44.67707824707031\n",
      "epoch:  12 loss:  36.903133392333984\n",
      "epoch:  13 loss:  28.477563858032227\n",
      "epoch:  14 loss:  19.74614715576172\n",
      "epoch:  15 loss:  11.34184741973877\n",
      "epoch:  16 loss:  4.347230911254883\n",
      "epoch:  17 loss:  0.35649147629737854\n",
      "epoch:  18 loss:  1.173837661743164\n",
      "epoch:  19 loss:  6.601728439331055\n",
      "epoch:  20 loss:  12.100554466247559\n",
      "epoch:  21 loss:  13.481314659118652\n",
      "epoch:  22 loss:  11.001201629638672\n",
      "epoch:  23 loss:  6.9084601402282715\n",
      "epoch:  24 loss:  3.1957380771636963\n",
      "epoch:  25 loss:  0.8519298434257507\n",
      "epoch:  26 loss:  0.028546540066599846\n",
      "epoch:  27 loss:  0.3459049165248871\n",
      "epoch:  28 loss:  1.2651304006576538\n",
      "epoch:  29 loss:  2.3297810554504395\n",
      "epoch:  30 loss:  3.223212480545044\n",
      "epoch:  31 loss:  3.769152879714966\n",
      "epoch:  32 loss:  3.903836250305176\n",
      "epoch:  33 loss:  3.6473464965820312\n",
      "epoch:  34 loss:  3.076427459716797\n",
      "epoch:  35 loss:  2.3070342540740967\n",
      "epoch:  36 loss:  1.480037808418274\n",
      "epoch:  37 loss:  0.7434433102607727\n",
      "epoch:  38 loss:  0.22815650701522827\n",
      "epoch:  39 loss:  0.01738925091922283\n",
      "epoch:  40 loss:  0.11683288961648941\n",
      "epoch:  41 loss:  0.4411480128765106\n",
      "epoch:  42 loss:  0.834301769733429\n",
      "epoch:  43 loss:  1.1268805265426636\n",
      "epoch:  44 loss:  1.206070065498352\n",
      "epoch:  45 loss:  1.0579535961151123\n",
      "epoch:  46 loss:  0.758665144443512\n",
      "epoch:  47 loss:  0.4259128272533417\n",
      "epoch:  48 loss:  0.1639697402715683\n",
      "epoch:  49 loss:  0.029150625690817833\n",
      "epoch:  50 loss:  0.023755596950650215\n",
      "epoch:  51 loss:  0.11063063144683838\n",
      "epoch:  52 loss:  0.2354598194360733\n",
      "epoch:  53 loss:  0.34675538539886475\n",
      "epoch:  54 loss:  0.40878549218177795\n",
      "epoch:  55 loss:  0.4069414436817169\n",
      "epoch:  56 loss:  0.34700295329093933\n",
      "epoch:  57 loss:  0.2501540184020996\n",
      "epoch:  58 loss:  0.14538221061229706\n",
      "epoch:  59 loss:  0.06080332770943642\n",
      "epoch:  60 loss:  0.015635311603546143\n",
      "epoch:  61 loss:  0.01487716194242239\n",
      "epoch:  62 loss:  0.048550259321928024\n",
      "epoch:  63 loss:  0.0962042585015297\n",
      "epoch:  64 loss:  0.13526101410388947\n",
      "epoch:  65 loss:  0.14978520572185516\n",
      "epoch:  66 loss:  0.13580630719661713\n",
      "epoch:  67 loss:  0.10105317085981369\n",
      "epoch:  68 loss:  0.05992312356829643\n",
      "epoch:  69 loss:  0.026649197563529015\n",
      "epoch:  70 loss:  0.009931505657732487\n",
      "epoch:  71 loss:  0.010872927494347095\n",
      "epoch:  72 loss:  0.024200333282351494\n",
      "epoch:  73 loss:  0.04147563502192497\n",
      "epoch:  74 loss:  0.05465855076909065\n",
      "epoch:  75 loss:  0.058755841106176376\n",
      "epoch:  76 loss:  0.052929285913705826\n",
      "epoch:  77 loss:  0.04004306718707085\n",
      "epoch:  78 loss:  0.02507644146680832\n",
      "epoch:  79 loss:  0.013051283545792103\n",
      "epoch:  80 loss:  0.0072167981415987015\n",
      "epoch:  81 loss:  0.008079282008111477\n",
      "epoch:  82 loss:  0.013566561974585056\n",
      "epoch:  83 loss:  0.020176293328404427\n",
      "epoch:  84 loss:  0.024565057829022408\n",
      "epoch:  85 loss:  0.0248727947473526\n",
      "epoch:  86 loss:  0.021246878430247307\n",
      "epoch:  87 loss:  0.01544586569070816\n",
      "epoch:  88 loss:  0.009825765155255795\n",
      "epoch:  89 loss:  0.006262259092181921\n",
      "epoch:  90 loss:  0.00549614941701293\n",
      "epoch:  91 loss:  0.007016611751168966\n",
      "epoch:  92 loss:  0.009512616321444511\n",
      "epoch:  93 loss:  0.011545038782060146\n",
      "epoch:  94 loss:  0.012141362763941288\n",
      "epoch:  95 loss:  0.01109764352440834\n",
      "epoch:  96 loss:  0.00892547331750393\n",
      "epoch:  97 loss:  0.006523511838167906\n",
      "epoch:  98 loss:  0.004746364429593086\n",
      "epoch:  99 loss:  0.004057893995195627\n",
      "39.873939514160156\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TEXT BERT CLASSFICIER\"\"\"\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "Device =torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class NLP(nn.Module):\n",
    " def __init__(self):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.Linear_sec = nn.Sequential(\n",
    "        nn.Linear(768,1),\n",
    "    )\n",
    "\n",
    " def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:,0,:]\n",
    "        logits = self.Linear_sec(cls_output)\n",
    "        return logits\n",
    " \n",
    "model = NLP().to(Device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "def tokinize_func(tr_words):\n",
    "    encoding = tokenizer(tr_words,return_tensors='pt',padding = True)\n",
    "    input_ids= encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    return input_ids.to(Device),attention_mask.to(Device)\n",
    "\n",
    "\n",
    "# Увеличенный набор данных с положительными и отрицательными комментариями о \"Сонике\"\n",
    "train_words = [\n",
    "\n",
    "\n",
    "    \"Я люблю человека\", \n",
    "    \"Человек очень весел\", \n",
    "    \"Человек великолепен\", \n",
    "    \"Человек имбовый чел\", \n",
    "    \"Человек лучший персонаж\",\n",
    "\n",
    "    \"Человек всегда радует\", \n",
    "    \"Человек это классика\", \n",
    "    \"Человек мой любимый герой\", \n",
    "    \"Человек это круто\", \n",
    "    \"Человек это весело\", \n",
    "\n",
    "    \"Человек супер\", \n",
    "    \"Человек это просто огонь\", \n",
    "    \"Человек это моя детская любовь\", \n",
    "    \"Человек это нечто\", \n",
    "    \"Человек это просто бомба\", \n",
    "\n",
    "# Отрицательные комментарии\n",
    "    \"Человек скучный\", \n",
    "    \"скин человека не привлекателен\", \n",
    "    \"Человек переоценен\", \n",
    "    \"Человек это дерьмо\", \n",
    "    \"Человек полный провал\", \n",
    "\n",
    "    \"Человек это хрень\", \n",
    "    \"Человек неинтересный\", \n",
    "    \"Человек это просто бред\", \n",
    "    \"Человек отстой\", \n",
    "    \"Человек это позор\", \n",
    "\n",
    "    \"Человек это не для меня\", \n",
    "    \"Человек это просто ужасно\", \n",
    "    \"Человек это полное говно\", \n",
    "    \"Человек это просто невыносимо\",\n",
    "\n",
    "# Комментарии с оскорблениями\n",
    "    \"Человек даун\", \n",
    "    \"Человек мразь\", \n",
    "    \"Человек тупой\", \n",
    "    \"Человек дебил\", \n",
    "    \"Человек полный идиот\", \n",
    "\n",
    "    \"Человек это просто позор\", \n",
    "    \"Человек отстой как и ты\", \n",
    "    \"Человек ты тупой\", \n",
    "    \"Человек мразь а не герой\", \n",
    "    \"Человек даун который не умеет работать\", \n",
    "\n",
    "    \"Человек дебил который не знает что делать\", \n",
    "    \"Человек тупой персонаж\", \n",
    "    \"Человек мразь а не друг\", \n",
    "    \"Человек полный провал\", \n",
    "    \"Человек это просто ужасно\", \n",
    "    \"Человек Конченый самый конченный человек\"\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Метки: 1 для положительных комментариев, 0 для отрицательных\n",
    "labels = torch.tensor([\n",
    "$onиk DayN\n",
    "    \n",
    "    1, 1, 1, 1, 1, \n",
    "    1, 1, 1, 1, 1, \n",
    "    1, 1, 1, 1, 1,\n",
    "\n",
    "      # Положительные\n",
    "    0, 0, 0, 0, 0, \n",
    "    0, 0, 0, 0, 0, \n",
    "    0, 0, 0, 0, 0,\n",
    "\n",
    "      # Отрицательные\n",
    "    0, 0, 0, 0, 0, \n",
    "    0, 0, 0, 0, 0, \n",
    "    0, 0, 0, 0, 0,\n",
    "\n",
    "], dtype=float).unsqueeze(1).to(Device)\n",
    "\n",
    "#print(attention_mask.shape)\n",
    "#print(input_ids.shape)\n",
    "for epoch in range(30):\n",
    "       model.train()\n",
    "       optimizer.zero_grad()\n",
    "       output=model(*tokinize_func(train_words))\n",
    "       loss = criterion(output, labels)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       #print(loss.item())\n",
    "       #print(output)\n",
    "with torch.no_grad():\n",
    "    input_words = [\"соник хороший человек\"]\n",
    "\n",
    "    eval_outputs=model(*tokinize_func(input_words))\n",
    "    print(eval_outputs)\n",
    "    print(\"мяу\" if eval_outputs>3 else \"Сигма капибара\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5e6b3-317e-4aec-a54e-faaa9175f43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
